{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Grains : Slice and Dice \n",
    "\n",
    "We use EfficientSAM in our workflow:\n",
    "\n",
    "1. Image is sliced into smaller windows\n",
    "2. Each slice is run through EfficientSAM to generate the annotations\n",
    "3. Count of annotations from each image is added\n",
    "4. ..finally the slices are stiched together\n",
    "\n",
    "This script provides example for how to get segment everything visualization result from EfficientSAM using weight file.\n",
    "\n",
    "The basic method is same as SAM, we generate a grid of point prompts over the image and get the masks. Currently we directly compute all the masks in one time so it requires a large memory. If you face OOM Issue, you can consider reduce the GRID_SIZE. We will update the efficient version by calculating the mask in local crops in the future.\n",
    "\n",
    "the post processing part is from original SAM project to get a better visualization result, part of the visualization code are borrow from MobileSAM project, many thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import tqdm \n",
    "GRID_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything.utils.amg import (\n",
    "    batched_mask_to_box,\n",
    "    calculate_stability_score,\n",
    "    mask_to_rle_pytorch,\n",
    "    remove_small_regions,\n",
    "    rle_to_mask,\n",
    ")\n",
    "from torchvision.ops.boxes import batched_nms, box_area\n",
    "def process_small_region(rles):\n",
    "        new_masks = []\n",
    "        scores = []\n",
    "        min_area = 100\n",
    "        nms_thresh = 0.7\n",
    "        for rle in rles:\n",
    "            mask = rle_to_mask(rle[0])\n",
    "\n",
    "            mask, changed = remove_small_regions(mask, min_area, mode=\"holes\")\n",
    "            unchanged = not changed\n",
    "            mask, changed = remove_small_regions(mask, min_area, mode=\"islands\")\n",
    "            unchanged = unchanged and not changed\n",
    "\n",
    "            new_masks.append(torch.as_tensor(mask).unsqueeze(0))\n",
    "            # Give score=0 to changed masks and score=1 to unchanged masks\n",
    "            # so NMS will prefer ones that didn't need postprocessing\n",
    "            scores.append(float(unchanged))\n",
    "\n",
    "        # Recalculate boxes and remove any new duplicates\n",
    "        masks = torch.cat(new_masks, dim=0)\n",
    "        boxes = batched_mask_to_box(masks)\n",
    "        keep_by_nms = batched_nms(\n",
    "            boxes.float(),\n",
    "            torch.as_tensor(scores),\n",
    "            torch.zeros_like(boxes[:, 0]),  # categories\n",
    "            iou_threshold=nms_thresh,\n",
    "        )\n",
    "\n",
    "        # Only recalculate RLEs for masks that have changed\n",
    "        for i_mask in keep_by_nms:\n",
    "            if scores[i_mask] == 0.0:\n",
    "                mask_torch = masks[i_mask].unsqueeze(0)\n",
    "                rles[i_mask] = mask_to_rle_pytorch(mask_torch)\n",
    "        masks = [rle_to_mask(rles[i][0]) for i in keep_by_nms]\n",
    "        return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_given_embeddings_and_queries(img, points, point_labels, model):\n",
    "    predicted_masks, predicted_iou = model(\n",
    "        img[None, ...], points, point_labels\n",
    "    )\n",
    "    sorted_ids = torch.argsort(predicted_iou, dim=-1, descending=True)\n",
    "    predicted_iou_scores = torch.take_along_dim(predicted_iou, sorted_ids, dim=2)\n",
    "    predicted_masks = torch.take_along_dim(\n",
    "        predicted_masks, sorted_ids[..., None, None], dim=2\n",
    "    )\n",
    "    predicted_masks = predicted_masks[0]\n",
    "    iou = predicted_iou_scores[0, :, 0]\n",
    "    index_iou = iou > 0.7\n",
    "    iou_ = iou[index_iou]\n",
    "    masks = predicted_masks[index_iou]\n",
    "    score = calculate_stability_score(masks, 0.0, 1.0)\n",
    "    score = score[:, 0]\n",
    "    index = score > 0.9\n",
    "    score_ = score[index]\n",
    "    masks = masks[index]\n",
    "    iou_ = iou_[index]\n",
    "    masks = torch.ge(masks, 0.0)\n",
    "    return masks, iou_\n",
    "\n",
    "def run_everything_ours(img_path, model):\n",
    "    model = model.cpu()\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = ToTensor()(image)\n",
    "    _, original_image_h, original_image_w = img_tensor.shape\n",
    "    xy = []\n",
    "    for i in range(GRID_SIZE):\n",
    "        curr_x = 0.5 + i / GRID_SIZE * original_image_w\n",
    "        for j in range(GRID_SIZE):\n",
    "            curr_y = 0.5 + j / GRID_SIZE * original_image_h\n",
    "            xy.append([curr_x, curr_y])\n",
    "    xy = torch.from_numpy(np.array(xy))\n",
    "    points = xy\n",
    "    num_pts = xy.shape[0]\n",
    "    point_labels = torch.ones(num_pts, 1)\n",
    "    with torch.no_grad():\n",
    "      predicted_masks, predicted_iou = get_predictions_given_embeddings_and_queries(\n",
    "              img_tensor.cpu(),\n",
    "              points.reshape(1, num_pts, 1, 2).cpu(),\n",
    "              point_labels.reshape(1, num_pts, 1).cpu(),\n",
    "              model.cpu(),\n",
    "          )\n",
    "    rle = [mask_to_rle_pytorch(m[0:1]) for m in predicted_masks]\n",
    "    predicted_masks = process_small_region(rle)\n",
    "    return predicted_masks, predicted_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_anns_ours(mask, ax):\n",
    "    ax.set_autoscale_on(False)\n",
    "    img = np.ones((mask[0].shape[0], mask[0].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in mask:\n",
    "        m = ann\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'EfficientSAM' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/yformer/EfficientSAM.git\n",
    "import os\n",
    "os.chdir(\"EfficientSAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_sam.build_efficient_sam import build_efficient_sam_vits\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"EfficientSAM/weights/efficient_sam_vits.pt.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"weights\")\n",
    "efficient_sam_vits_model = build_efficient_sam_vits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slice the image into smaller units\n",
    "\n",
    "```\n",
    "$grains.size\n",
    "(3024, 4032)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"E:\\EfficientSAM\\grains_of_sand.jpg\"\n",
    "grains = Image.open(filepath)\n",
    "\n",
    "if not os.path.exists(\"grain_slices\"):\n",
    "    os.mkdir(\"grain_slices\")\n",
    "\n",
    "for r in range(0, grains.size[0],520):\n",
    "    for c in range(0, grains.size[1],520):\n",
    "        box = (r,  c, r+520, c+520)\n",
    "        # print(box)\n",
    "        grain_slice = grains.crop(box)\n",
    "        grain_slice.save(f\"grain_slices\\\\grains_{r}_{c}.jpg\")\n",
    "\n",
    "# get each file name\n",
    "        \n",
    "filenames = os.listdir(\"grain_slices\")\n",
    "filenames = [fp for fp in filenames if fp.endswith(\".jpg\")]\n",
    "\n",
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"grain_masks\"):\n",
    "    os.mkdir(\"grain_masks\")\n",
    "\n",
    "mask_count = 0\n",
    "\n",
    "for image_path in tqdm.tqdm(filenames):\n",
    "    \n",
    "    #Create output filename\n",
    "    output_file = image_path.split(\".\")[0] + \"_mask.jpg\"\n",
    "    output_file = os.path.join(\"grain_masks\", output_file)\n",
    "\n",
    "    # Run SAM\n",
    "    image_path = os.path.join(\"grain_slices\", image_path )\n",
    "    mask_efficient_sam_vits, mask_iou = run_everything_ours(image_path, efficient_sam_vits_model)\n",
    "\n",
    "    mask_count += len(mask_iou) -1 \n",
    "    #Generate output mask file\n",
    "    width_px = 520\n",
    "    height_px = 520 \n",
    "    dpi = 96\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(width_px/dpi, height_px/dpi), dpi=dpi)\n",
    "\n",
    "    image = np.array(Image.open(image_path))\n",
    "    ax.imshow(image)\n",
    "    show_anns_ours(mask_efficient_sam_vits, ax)\n",
    "    ax.axis('off')\n",
    "    plt.savefig(output_file, bbox_inches=\"tight\", pad_inches=0, dpi=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stitch the slices together into a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_width, result_height = Image.open(\"E:\\\\EfficientSAM\\\\grains_of_sand.jpg\").size\n",
    "result_image = Image.new(\"RGB\", (result_width, result_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total masks files: 48\n"
     ]
    }
   ],
   "source": [
    "width_px = 520\n",
    "height_px = 520 \n",
    "dpi = 96\n",
    "\n",
    "filenames = os.listdir(\"grain_masks\")\n",
    "filenames = [fp for fp in filenames if fp.endswith(\".jpg\")]\n",
    "\n",
    "print(f\"Total masks files: {len(filenames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in filenames: \n",
    "    # Find the upper-left box point for each image slice\n",
    "    file_box = file.replace(\"grains_\",\"\")\n",
    "    file_box = file_box.replace(\"_mask.jpg\", \"\")\n",
    "    box = [int(fp) for fp in file_box.split(\"_\")]\n",
    "\n",
    "    # Read image file \n",
    "    img = Image.open(os.path.join(\"grain_masks\",file))\n",
    "    img = img.resize((520,520))\n",
    "    # Stich together the images    \n",
    "    result_image.paste(im=img, box=box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_image.save(\"grain_mask.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of grains: 39690\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of grains: {mask_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
